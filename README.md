# MGL7811 â€“ German Credit Report (Docker + NoSQL)

****Auteur : Abdoul Karime DIOP****

****Code Permanent: DIOA14279808
****GitHub : https://github.com/karimoulah********

---

## ğŸ¯ Objectif du projet

Mettre en place un environnement **reproductible** pour exÃ©cuter le notebook **`MGL7811_GermanCreditReport.ipynb`** avec les contraintes suivantes :

- Tous les composants (Jupyter/Notebook, etc.) sâ€™exÃ©cutent en **conteneurs Docker** ;
- Le dataset nâ€™est **pas lu directement depuis un CSV** par le notebook : il provient dâ€™une **base NoSQL (MongoDB)** ;
- Un Ã©valuateur doit pouvoir **reproduire** et exÃ©cuter lâ€™application en quelques commandes.

---

## ğŸ§± Architecture & flux

```
mgl7811_project/
â”œâ”€ docker-compose.yml
â”œâ”€ README.md
â”œâ”€ .gitignore
â”œâ”€ data/                       # (Ã  VOUS de dÃ©poser german_credit_data.csv ici)
â””â”€ jupyter/
   â”œâ”€ Dockerfile               # image Jupyter (Python 3.11 + data stack + pymongo)
   â”œâ”€ requirements.txt         # versions figÃ©es (pandas, sklearn, matplotlib, ...)
   â””â”€ notebooks/
      â”œâ”€ MGL7811_GermanCreditReport.ipynb
```

**Services (docker-compose.yml)**

- `mongo` : **MongoDB 6.0** (stockage NoSQL, persistance via volume `mongo_data`)
- `mongo-express` : UI web dâ€™administration MongoDB (port `8081`)
- `jupyter` : Environnement **Jupyter Lab** (port `8888`) avec dÃ©pendances data science + `pymongo`
- `data-loader` : Conteneur Ã©phÃ©mÃ¨re qui **importe** `./data/german_credit_data.csv` â†’ MongoDB (`db=german_credit_data`, `collection=records`)

**Flux de donnÃ©es**

1. Vous dÃ©posez `german_credit_data.csv` dans `./data/`
2. `data-loader` exÃ©cute `mongoimport` â†’ documents insÃ©rÃ©s dans MongoDB
3. Le notebook **lit la collection** via `utils_mongo_loader.load_from_mongo()` â†’ `dataset` (pandas DataFrame)
4. Les analyses sâ€™exÃ©cutent **sans dÃ©pendre du CSV** (source = NoSQL).

**Raisons de lâ€™architecture**

- Respect strict de la contrainte **NoSQL** ;
- Import **idempotent** et reproductible sÃ©parÃ© de lâ€™analyse ;
- **mongo-express** facilite la vÃ©rification rapide du contenu ;
- **Versions figÃ©es** des bibliothÃ¨ques pour garantir les rÃ©sultats ;
- SÃ©paration claire des responsabilitÃ©s (DB / import / notebook).

---

## âš™ï¸ PrÃ©requis

- **Docker** â‰¥ 20.x et **Docker Compose** (plugin) â‰¥ v2.x
- **Git** pour cloner le dÃ©pÃ´t
- OS cible : Linux, macOS, ou Windows (**WSL2** recommandÃ©)

> VÃ©rifiez Docker : `docker --version` et `docker compose version`

---

## ğŸš€ DÃ©marrage rapide (Quickstart)

1) **Cloner** le dÃ©pÃ´t et se placer dans le dossier :

```bash
git clone https://github.com/karimoulah/Projet1_Machine_Learning.git
```

2) **Placer le dataset** dans `./data/` :

- Fichier attendu : **`german_credit_data.csv`**
- Format : **ligne dâ€™en-tÃªtes** + sÃ©parateur virgule `,`

3) **Lancer lâ€™environnement** :

```bash
docker compose up -d --build
```

4) **AccÃ©der aux interfaces** :

- Jupyter Lab : http://localhost:8888/lab
- Mongo Express : http://localhost:8081/

5) **Ouvrir et exÃ©cuter** le notebook :
   `jupyter/notebooks/MGL7811_GermanCreditReport.ipynb`

> La 1Ã¨re cellule lit Mongo et crÃ©e `dataset` (pandas DataFrame).

---

## ğŸ”§ Configuration & variables dâ€™environnement

- Le service **`jupyter`** reÃ§oit :
  - `MONGO_HOST=mongo`
  - `MONGO_PORT=27017`
- **Volumes** :
  - `./jupyter/notebooks` â†” `/home/jovyan/work`
  - `./data` â†” `/home/jovyan/data`
  - `mongo_data` (volume Docker) pour la persistance DB

**Ports par dÃ©faut** :

- Jupyter : `8888`
- Mongo Express : `8081`
- MongoDB : `27017`

> Modifiez-les si des conflits existent (voir *DÃ©pannage*).

---

## ğŸ“Š DonnÃ©es : format & import

- Le service `data-loader` exÃ©cute en interne :
  ```bash
  mongoimport --host mongo \
              --db german_credit_data \
              --collection records \
              --type csv --headerline \
              --file /data/import/german_credit_data.csv \
              --drop
  ```
- **Important** : la **premiÃ¨re ligne** du CSV doit contenir les **noms de colonnes**.
- Pour **rÃ©importer** aprÃ¨s modification du CSV :
  ```bash
  docker compose run --rm data-loader
  ```

---

## ğŸ§ª Notebook : contenu & bonnes pratiques

Le notebook couvre les points typiques dâ€™une analyse MGL7811 :

- **Partie 1 : Exploration** â€“ taille du dataset, types de variables, distribution de la cible `Risk`
- **Partie 2 : Distributions & relations** â€“ visualisations (countplot, hist, hue=Risk), corrÃ©lations pertinentes
- **Partie 3 : PrÃ©paration (feature engineering)** â€“ encodage catÃ©goriel, gestion NaN, split train/testxs

**Bonnes pratiques** :

- Conservez les transformations **dans le notebook**, pas dans des scripts externes ;
- Logguez la **version** des libs (pandas, numpy, sklearn) ;
- Commentez les hypothÃ¨ses et choix (encodage, traitement des NaN, etc.).

---

## ğŸ”’ SÃ©curitÃ© (local)

- Jupyter dÃ©marre **sans token** pour simplifier la correction locale. **Ne pas** exposer ce port publiquement.
- Pour activer la protection : remplacez la `CMD` dans `jupyter/Dockerfile` par la commande par dÃ©faut de Jupyter et dÃ©finissez un **token**/**mot de passe**.
- Mongo Express est une UI dâ€™**administration** ; Ã©vitez de lâ€™ouvrir sur Internet.

---

## ğŸ§­ Organisation Git (exigence pÃ©dagogique)

- Travail quotidien sur **`dev`** ;
- **`main`** ne contient que le **livrable final propre** (pas dâ€™anciens brouillons / fichiers inutiles) ;
- `data/` est **ignorÃ©** (pas de donnÃ©es dans Git).

## â™»ï¸ ReproductibilitÃ© & dÃ©terminisme

- **Versions figÃ©es** dans `jupyter/requirements.txt` ;
- **Import idempotent** via `data-loader` ;
- Notebook autonome : **source = MongoDB** (aucune dÃ©pendance lecture CSV pendant lâ€™exÃ©cution).

---

## ğŸ§© Variantes dâ€™architecture (selon ressources)

- **Machine modeste** :

  - Laisser lâ€™architecture telle quelle (3 services + loader).
  - Si nÃ©cessaire, **arrÃªter `mongo-express`** (facultatif) pour Ã©conomiser de la RAM.
- **Machine puissante** :

  - Ajouter un service **scheduler**/CI (GitHub Actions) pour exÃ©cuter des _smoke tests_ (exÃ©cuter une cellule de lecture Mongo).
  - Ã‰tendre vers dâ€™autres NoSQL (Cassandra, CouchDB) pour comparaison (adapter `data-loader` et `utils_mongo_loader.py`).

---

## âœ… Checklist de validation (pour le correcteur)

- [ ] `docker compose up -d --build` fonctionne sans erreur
- [ ] Le CSV est ignorÃ© par Git (`data/` dans `.gitignore`)
- [ ] `data-loader` importe bien vers Mongo (`db=german_credit_data`, `collection=records`)
- [ ] **Notebook** lit **Mongo** (et non un CSV) et crÃ©e `dataset`
- [ ] Analyses/visualisations **sâ€™exÃ©cutent** et sont **pertinentes**
- [ ] README prÃ©sent, clair, avec explication dâ€™architecture et **nom de lâ€™auteur**
- [ ] Branche **`main`** propre, **`dev`** pour lâ€™historique

---

## ğŸ› ï¸ DÃ©pannage (FAQ technique)

- **Port dÃ©jÃ  utilisÃ©** (`8888`, `8081`, `27017`)â†’ Modifier `docker-compose.yml` (ex. `8889:8888`), relancer `make up`
- **`Permission denied` sur Docker (Linux)**â†’ Ajouter votre utilisateur au groupe `docker` puis se reconnecter
- **`File not found: german_credit_data.csv`**â†’ DÃ©poser le fichier dans `./data/`, relancer `make reimport` ou `make up`
- **`Headerline` manquant** (import CSV)â†’ VÃ©rifier que la **1Ã¨re ligne** du CSV contient bien les **en-tÃªtes**
- **Notebook ne voit pas les donnÃ©es**
  â†’ VÃ©rifier `MONGO_HOST=mongo`, `MONGO_PORT=27017` cÃ´tÃ© service `jupyter`
